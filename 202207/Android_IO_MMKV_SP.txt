SP、DataStore、MMKV对比
扔无线：https://www.bilibili.com/video/BV1FU4y197dL?spm_id_from=333.999.0.0 

DataStore
解决SP的两个问题
  1、性能问题
	DataStore读跟写都是使用协程在子线程中完成的。
  2、回调问题
	场景：等数据写入之后再做某某事情
	使用SP的commit同步写入会卡主线程
	使用SP的apply异步写入你还需要设计回调机制
	DataStore使用协程线程切换是非常简单的
缺点：需要在kotlin、协程的支持下才能使用 


MMKV
优点：
  1、极高的同步写入磁盘速度
  2、高性能的高频写入小数据
  3、支持多进程
  4、进程崩溃不会影响写入
缺点：
  1、系统崩溃造成文件损坏的情况下会有可能导致丢数据。(SP、DataStore有内存缓存，不会出现这个问题)
  所以如果数据重要，需要自己手动备份数据。 
  2、写入大数据、初次加载文件（特别是文件数据特别大）会有可能造成卡顿的





MMKV（mmap + 文件锁）https://cloud.tencent.com/developer/article/1354199
IPC选择：
	中心化框架：Binder（CS架构，缺点：启动慢（需要Server端的启动）、访问慢（各种鉴权、各种切binder线程））、其他框架（ socket、PIPE等就更慢了，需要两次拷贝）
	去中心化框架：既然用了mmap，它就支持进程通信了，进程间同步通过进程锁实现就好了。
进程锁选择：
	pthread_mutex：支持递归锁、锁升级降级。不支持进程退出自动释放锁，需要自行释放（可能会出现饿死）。
	文件锁（fcntl）：不支持进程退出自动释放锁。但是不支持递归锁、锁升级降级，需要自行实现。

实现细节（多进程的状态同步问题）：
文件头部保存有状态信息，各个进程也缓存自己的状态信息。如果发现缓存与文件头部的不一致，说明有进程写入了，则进程需要更新缓存。
写指针感知：
	保存到状态信息，进程在写入后都把写指针更新到状态信息里边。
	其他进程获得写锁之后，读取下状态信息的写指针是否与自己缓存的一致，即可感知到。
内存重整感知：
	使用一个单调递增的序列号，每次发生内存重整将序列号递增，并保存到状态信息中。
内存增长感知：
	无需额外保存，获取文件大小即可感知。

文件锁的完善：
支持递归锁：
	什么是递归锁：一个进程/线程已经拥有了锁，那么后续的加锁操作不会导致卡死，并且解锁也不会导致外层的锁被解掉。对于文件锁来说，前者是满足后者则不满足。因为文件锁是状态锁，没有计数器，无论加了多少次锁，一个解锁操作就全解掉。

	解决方法：增加锁计数

支持升级、降级锁：
	什么是锁升级：已经持有的共享锁（读锁），升级为互斥锁（写锁）。
	什么是锁降级：锁降级则是反过来。
	文件锁支持锁升级但是容易死锁：假如A、B进程都持有了读锁，现在都想升级到写锁，就会陷入相互等待的困境，发生死锁。
	另外由于文件锁不支持递归锁，也导致了锁降级无法进行，一降就降到没有锁。

	解决方法：
	加写锁时，如果当前已经持有读锁，那么先尝试加写锁，try_lock 失败说明其他进程持有了读锁，我们需要先将自己的读锁释放掉，再进行加写锁操作，以避免死锁的发生。
	
	解写锁时，假如之前曾经持有读锁，那么我们不能直接释放掉写锁，这样会导致读锁也解了。我们应该加一个读锁，将锁降级。







SP
性能问题 - 卡顿现象（严重甚至会发生ANR）

问题1、在SP创建的时候，会开一个线程去解析sp文件并将sp内容加载到内存中。如果在解析的过程，主线程尝试访问SP就会被Block，知道解析完成。
SharedPreferencesImpl构造函数 -> startLoadFromDisk
	private void startLoadFromDisk() {
		//mLoaded设置false，sp文件就绪就会设置为true。并且mLock.notifyAll
        synchronized (mLock) { mLoaded = false; }
        new Thread("SharedPreferencesImpl-load") { ... }.start();
    }

SP的所有操作，都会先获取锁，然后再判断mLoaded
	synchronized (mLock) {
        awaitLoadedLocked();
        ...
    }

    private void awaitLoadedLocked() {
        if (!mLoaded) {
            BlockGuard.getThreadPolicy().onReadFromDisk();
        }
        while (!mLoaded) {
            try {
                mLock.wait(); //如果sp没就绪，那么这里就会进入Block
            } catch (InterruptedException unused) { }
        }
        ...
    }



问题2、commit同步写入肯定是阻塞的。apply异步写入就不会阻塞了吗？
SP 调用 apply 方法，会创建一个等待锁放到 QueuedWork 中，并将真正数据持久化封装成一个任务放到异步队列中执行，任务执行结束会释放锁。
执行 QueuedWork.waitToFinish() 会等待所有的等待锁释放。
太多 pending 的 apply 行为没有写入到文件，主线程在执行到QueuedWork#waitToFinish的时候会有等待行为，会造成卡顿甚至出现ANR

apply的流程:
1、首先调用commitToMemory将数据改动同步到内存中，也就是SharedPreferencesImpl的mMap

2、将一个awaitCommit的Runnable加入到QueuedWork的finisher队列中
   这个awaitCommit的Runnable的逻辑是 mcr.writtenToDiskLatch.await(); 等待这次mMap的文件写入完成。

3、最后将执行mMap写入文件的逻辑封装在一个Runnable里边，并将其添加到QueuedWork的工作队列中（QueuedWork#queue）
   在文件写入完成之后，会执行writtenToDiskLatch.countDown() 唤醒等待并且移除掉finisher队列中的awaitCommit。

4、QueuedWork#queue 处了将Runnable加入到工作队列之外，还会进行sendMsg操作。让QueuedWork内部的HandleThread工作起来，
   执行工作队列中的所有Runnable。

QueuedWork：执行异步任务的工具类，内部的实现逻辑的就是创建一个HandlerThread作为工作线程，然后QueuedWorkHandler和这个HandlerThread进行管理，每当有任务添加进来就在这个异步线程中执行，这个异步线程的名字queued-work-looper

ActivityThread在处理组件生命周期中，分别有4处地方调用了 QueuedWork#waitToFinish
分别是：Activity#onPause、Activity#onStop、Service#onStartCommand、Service#onDestroy

waitToFinish：（这个方法是执行在主线程的）
1、如果工作队列还有Runnable，继续执行完它们
2、如果finisher队列有Runnable，执行完它们
   问题就出在这：
   apply的中写入操作也是在异步线程执行，不会导致主线程卡顿。
   但是如果异步任务执行时间过长，当ActvityThread执行到了QueuedWork#waitToFinish 
   就会进入 awaitCommit这个Runnable里边的 await 等待。


问题1解决：采用预加载的方式。
真正需要处理的是核心场景的SP一定不能太大，官方的声明还是有必要遵守一下.

问题2解决：hook掉finisher队列，让其poll方法返回null。


IO

页缓存（page cache）：文件读写并不是每次都进行磁盘IO，而是将对应的磁盘文件缓存到内存上，之后对该文件的操作实际上也是对内存的读写。而这个缓存就是页缓存。

脏页（dirty page）：被修改过但还没写入磁盘的页缓存称为脏页。

read过程：读取文件时，操作系统会先从缓存中查找对应文件。
	如果有对应缓存 -> 直接读取缓存内容。
	如果没有对应缓存 -> 产生缺页中断，将文件读取到缓存中，同时read过程也会被阻塞。

write过程：写入数据时，会先将数据写入缓存。
	如果有对应缓存 -> 直接写入缓存，标记为脏页。
	如果没有对应缓存 -> 产生缺页中断，将磁盘文件读取到缓存，再修改缓存内容，标记为脏页。

磁盘回写：
定时回写：由pdflush进程定时将脏页写入磁盘（周期为/proc/sys/vm/dirty_writeback_centisecs，单位是(1/100)厘秒）
fsync()回写：这时候系统会唤醒pdflush进程进行回写，直到所有的脏页都写到磁盘为止。
内存不足或脏页过多，write系统调用会同样会唤醒pdflush进程进行回写。

所以传统的读写：
读：file（内核空间）-> page cache（内核空间）-> user buffer（用户空间）---> 访问
写：修改 ---> user buffer（用户空间）-> page cache（内核空间）-> file（内核空间）
每个 -> 都是一次拷贝，所以在没有页缓存的情况下读、写都需要两次拷贝

mmap
如果可以直接在用户空间读写页缓存，那么就可以免去将页缓存的数据复制到用户空间缓冲区的过程。变成
读：file（内核空间）-> page cache（内核空间）---> 访问
写：修改 ---> page cache（内核空间）-> file（内核空间）

mmap：虚拟内存地址 --映射--> page cache -> file

内核并不会主动把mmap映射的页缓存同步到磁盘，而是需要用户主动触发。同步mmap映射的内存到磁盘有4个时机：
调用 msync 函数主动进行数据同步（主动）。
调用 munmap 函数对文件进行解除映射关系时（主动）。
进程退出时（被动）。
系统关机时（被动）。